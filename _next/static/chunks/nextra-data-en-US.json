{"/architecture-experience":{"title":"Architecture Experience","data":{"시스템-설계-경험#시스템 설계 경험":""}},"/architecture-experience/consumer-thread-optimization":{"title":"Consumer Thread Optimization","data":{"스레드-최적화#스레드 최적화":"흔히 트래픽이 높으면 스레드를 높여야 한다는 착각에 빠질 수 있습니다. 하지만 개발을 해보면서 느낀 점은 각각의 로직이 비동기적으로 논블로킹연산이 되도록 구현을 하면 스레드 개수가 그렇게 많을 필요가 없었다는 점이었습니다. 이렇게 정의한 비동기 논블로킹 연산의 람다 바디 정의부는 캐시에 데이터를 쌓아두는 생산자와 캐시의 데이터를 일정 사이즈 만큼 읽어들여서 소비하는 소비자를 함께 구현해두면, 서로가 서로의 작업을 블로킹하지 않고 별도의 ExecutorService 내에서 Async 하게 동작하기 때문에 애플리케이션 레벨에서의 교착현상은 사라집니다.제 경우에는 스케쥴러 Executor 와 작업을 수행하는 workerExecutor, RabbitMQ 의 데이터를 receive 하는 listenerExecutor 로 분류해서 각각을 관리했습니다.\nschedulerExecutor : 작업 조율 스레드\nworkerExecutor : 데이터 저장작업\npushExecutor : 데이터 Push 역할 담당\nlistenerExecutor : 데이터 수신 역할 담당\n레피니티브의 고빈도 트래픽을 받기 위해서는 적어도 2~3 배수의 비동기 작업이 필요했는데, 이런 이유로 listenerExecutor 를 min =1, max =2 으로 두어서 스레드 풀을 설정했습니다. 데이터 수신의 경우 데이터 수신 외에는 별도의 IO 작업이 없었고 캐시에 데이터를 적재하는 역할만을 수행하는데, 캐시에 데이터가 적재되는 속도를 직접 체크해본 결과 최대 3ns 안에 이루어졌기에 실제로는 그렇게 큰 스레드 풀이 필요하지는 않았습니다.데이터를 수신한 후 캐시에 쌓아둔 작업 대기열의 소비 작업은 schedulerExecutor 가 workerExecutor, pushExecutor 중 하나를 선택해서 해당 스레드를 실행시키도록 했습니다.","아쉬운-점#아쉬운 점":"최근 webflux, kotlin coroutine 스터디 문서를 작성하면서 \n\"자바의 스레드와 톰캣의 스레드 풀로 레피니티브의 고빈도 트래픽을 처리하는 건 정말 리소스를 비싸게 사용하는 것이었구나\"\n하는 생각을 했었습니다. 사실 그 당시 이 기능 개발 전 프로젝트 셋업 당시에 webflux 를 도입하려고 했지만 반대에 부딪혔었는데 만약 그 당시에 webflux 를 하겠다고 우기고 앞으로 나갔다면 지금은 어땠을까? 하고 생각해봤던 것 같습니다.\n스레드 하나에 작업 하나를 부여해서 그 작업을 비동기 논블로킹으로 연산했던 것들은 지금에 와서 돌아보면 \"IO작업 하나에 정말 너무 비싼 비용을 치룬 것이 아닌가\" 하는 생각을 너무나 자주 합니다. 톰캣의 스레드 풀 보다는 Netty 기반의 AIO 네트워킹 기반의 WAS 서버에서 프로젝트가 진행되었다면 조금은 프로젝트가 빨라졌지 않을까 하는 생각도 해봤던 것 같습니다."}},"/architecture-experience/if-project-again":{"title":"If Project Again","data":{"프로젝트를-다시-한다면#프로젝트를 다시 한다면":"","exactly-once-보장#Exactly Once 보장":"메시지를 통신 상으로 오직 한번만 받을 수 있음이 보장되는 Exactly Once 를 적용하지 못한 것은 아쉬운 점이라고 생각합니다.개발 당시에는 주식 시세 데이터 특성상 초/분/시/일 데이터에 대해서 고가/저가/현재가/종가만 계속해서 업데이트하면 되고  데이터의 중복보다는 업데이트가 더 중요했기에 Exactly Once 가 중요하지는 않았습니다.다시 프로젝트를 하게 될 경우, 통신 매커니즘 레벨에서는 1번만 메시지를 받을 수 있음을 보장하게끔 Exactly Once가 보장되도록 아래와 같은 시도를 할 것 같습니다. 알수 없는 장애 또는 비즈니스 요건상 장애 발생시\n카프카 내의 장애 토픽에 장애 데이터, 키값, 장애 발생처 등에 대한 정보를 적재\n장애 토픽이 정상 운영중이라면, 장애 처리 인스턴스에서 이것을 비즈니스로직에 따라 처리\n카프카의 장애 토픽이 응답할 수 없는 상태일 때 레디스에 기록\n레디스에 기록, AOF 기능 활용\n레디스가 응답할 수 없는 상태일 때 노드 내의 오프힙 캐시 저장소에 기록\n기록된 데이터는 주기적으로 동작하는 별도의 애플리케이션에서 알람,장애 기록 적재 등의 역할을 수행","상태없는-연산이-필요한-인스턴스에는-k8s-애플리케이션으로-전환#상태없는 연산이 필요한 인스턴스에는 k8s 애플리케이션으로 전환":"waiker-data-live\nwaiker-data-websocket\nwaiker-data-live, waiker-data-websocket 은 데이터를 Serving 하는 역할을 수행합니다. 각각의 데이터가 상태가 없이 개별 데이터 건을 전달하기만 하면 되는 역할을 갖고 있기 때문에 EKS 구축과 동시에 k8s 네이티브 애플리케이션으로 전환했을 것 같습니다.","비동기-논블로킹-지원-네트워크-프로그램으로-전환#비동기 논블로킹 지원 네트워크 프로그램으로 전환":"Netty 기반의 웹 애플리케이션으로 전환\nKotlin, Webflux 기반으로 전환\n스레드 하나에 작업 하나를 부여해서 그 작업을 비동기 논블로킹으로 연산했던 것들은 지금에 와서 돌아보면 \"IO작업 하나에 정말 너무 비싼 비용을 치룬 것이 아닌가\" 하는 생각을 너무나 자주 합니다. 톰캣의 스레드 풀 보다는 Netty 기반의 NIO 네트워킹, 이벤트 루프, 멀티플렉싱 기반 이벤트 채널링이 지원되는 WAS 서버에서 프로젝트가 진행되었다면 조금은 프로젝트가 빨라졌지 않을까 하는 생각도 해봤던 것 같습니다.","필드매핑-정합성-검증-서비스-개발#필드매핑 정합성 검증 서비스 개발":"당일 트래픽들을 카프카 토픽 컨슈머로 수집\n로그 적재 시스템 구축\n로그 데이터를 읽어들여서 시계열 데이터베이스에 배치 기반으로 데이터 적재\n장 마감 후 일괄 배치 작업을 통한 데이터 보정 작업 및 에러율 검출\n(in-house) 크롤링을 통한 타사 플랫폼 집계 수집 & 데이터와 비교했을 때 데이터 에러 구간 검출\n개발 기한과 개발 인력이 부족했기에 로그를 추출해서 이것을 기반으로 서비스가 과거에 잘 돌아갔었는지 시세 데이터의 정합성이 어느 정도인지 추측할 수 있는 in-house 툴을 개발할 필요가 있었습니다. 당시에는 1인 개발 체제여서 테스트 코드로만 남겨두었지만, 별도의 프로젝트를 개발할 인력이 있었다면 이렇게 로그 처리 시스템을 구축했을 듯 합니다.","kafka-spring-cloud-function#Kafka, Spring Cloud Function":"카프카 기반의 메시지 큐로 전환\nRabbitMQ, Kafka 등 메시징 솔루션에 종속되지 않도록 Cloud Function 을 이용한 동시성 처리 기능 개발\n개발 당시에 카프카 기반의 개발을 하겠다는 입장을 취했지만, 랩장님의 반대가 있었고 카프카를 운영환경에서 고도화 시킬만한 인프라팀의 데브옵스 엔지니어도 부족했습니다. 이런 이유로 클러스터링 기반의 레빗엠큐를 선택했습니다. 레빗엠큐가 성능적으로 전혀 부족한 면이 있는 것은 아니었지만, 카프카의 회복탄력성, 파티셔닝을 통한 확장성, 주키퍼와 브로커들간의 통신을 통해 통신의 신뢰성을 확보하는 등 여러가지 장점을 가지고 있기에 Kafka 기반의 메시징 시스템으로 전환했을 듯 합니다.","생산자소비자-기반-동시성-처리-구조-커스터마이징#생산자/소비자 기반 동시성 처리 구조 커스터마이징":"데이터 가 아닌 요청 을 기록하는 용도의 테이블을 따로 만들어서 기록을 하는 구조로 전환하게 될 것 같습니다. 동시성 처리가 되어 있는 생산자/소비자 기능은 개별 요청을 기록(Insert) 하고, 주기적으로 이 테이블에서 벌크단위로 기록된 요청 데이터를 읽어들여서 시세테이블에 데이터를 적재하도록 구조를 전환해서 동일행 수정으로 인한 DBMS에서의 잦은 MVCC 이슈를 피하고, 각각의 요청을 고유하게 기록할 수 있는 구조로 전환하게끔 해보고 싶습니다.","tsdb#TSDB":"1차적인 데이터 처리 용도의 데이터베이스는 관계형 데이터베이스 대신 시계열 데이터 기반의 데이터베이스를 적용했을 듯 합니다.관계형 데이터베이스를 억지로 시계열 데이터에 처음부터 맞추는 것은 억지스럽다는 생각은 했으나 당시 개발 여건 상 팀내의 입지나 여러모로 도입하기가 쉽지 않았습니다. 관계형 데이터베이스는 데이터를 중복을 없애고 일관성있는 논리적인 데이터의 구조를 통해 데이터를 효율적으로 저장하는 것이 목적이기에, 현재가/고가/저가/종가가 계속 변할 때마다 동시성 이슈가 크다고 생각했습니다.아마도 프로젝트를 다시 한다면 관계형 데이터베이스에는 최종적으로 데이터를 1분마다 집계한 최종 데이터를 저장하고, TSDB에 시세데이터를 증분기록하게끔 할 것 같습니다. 지금은 이렇게 적긴 했지만, 시세 데이터 자체가 관계형 데이테베이스에 저장한다는 것 자체가 저 자신은 가끔 이해가 안되긴 하지만, 팀내에서 이렇게 하기를 원한다면 관계형 DB에 저장을 하게 될 듯 합니다.TSDB(InfluxDB 등) 외에도 MongoDB, ElasticSearch 가 대안이 될수 있을 것 같고, 완전하게 독자적으로 책임질수 있는 여건이라면 관계형 Database 에 시세를 기록하지는 않을 듯 합니다. 대신 시세 조회시에는 API를 통해서 조회를 하게끔 하고, 시세 서비스 내에서는 NoSQL 데이터베이스를 사용하게끔 했을 듯 합니다.","eof#EOF":"이 당시에 여러 모로 팀 내에 강압적인 면도 많았고, 비인격적인 대우도 많았고, 알게 모르게 뒤에서 비웃는 분들도 많았던 회사였기에 프로젝트 진행을 위한 의사소통 자체가 너무 안되고 업무 외적으로 사적인 요소까지 이해할 수 없는 일들이 너무 많이 실제로 발생해서 힘들게 만들던 시절이었던 것 같습니다. 아마도 다시 프로젝트를 한다면 위의 요소들을 진지하게 고민을 하게 될 듯 합니다."}},"/architecture-experience/intro":{"title":"Intro","data":{"":"시스템을 직접 설계해서 MSA로 분해하고 각 서비스간 통신을 위해 메시지큐를 도입해나간 과정과 레피니티브의 시세 트래픽을 밀리지 않고 I/O 작업을 처리하기 위해 적용했던 설계방식, 문제해결 과정을 정리했습니다.","하위-페이지#하위 페이지":"메시지 큐 기반 시스템 분해작업\n스레드 최적화\n테스트케이스 경험\n프로젝트를 다시 한다면","프로젝트-설명#프로젝트 설명":"한국증권거래소 처럼 증권 시세를 Serving 하는 레피니티브라고 하는 회사의 시세 데이터를 1차 가공한 후 웹소켓 Push, Database 저장하는 것을 담당하는 시스템입니다.","트래픽-규모#트래픽 규모":"트래픽은 아래와 같이 2.5k/s ~ 7.5k/s 였습니다.\n개장(Market Open) 트래픽 추이\n폐장(Markt Close) 트래픽 추이","주요-문제#주요 문제":"2.5k/s ~ 7.5k/s 의 트래픽을 밀리지 않고 데이터를 저장하고, 웹소켓을 통해 웹/앱 으로 Push 를 해야 했습니다.웹소켓 푸시, 데이터 저장에 드는 I/O 작업의 시간 비용은 고정적으로 소모되므로, 일반적인 서버 애플리케이션의 처리방식과는 다른 접근방식이 필요했습니다. 또한, 요청이 유실 되었을 경우 이것을 복구해낼 방법이 필요했는데 이 부분에 대한 접근방식도 필요했습니다."}},"/architecture-experience/message-queue-usage":{"title":"Message Queue Usage","data":{"메시지-큐-기반-시스템-분해작업#메시지 큐 기반 시스템 분해작업":"","레거시-데이터-처리-서버---valuesight#레거시 데이터 처리 서버 - valuesight":"웨이커에서는 레거시 버전의 데이터 처리서버를 신규 버전으로 새로 개편을 해야 했습니다. valuesight 는 쉽게 설명하면 아래와 같은 역할을 합니다.\n레피니티브로부터 실시간으로 전달되는 주식 거래 raw 데이터를 수신\nraw 데이터에 대한 현재가, 52주신고가, BID, ASK, 고가, 저가 등의 필드 매핑\n웹소켓으로 현재가 Push\n매핑된 데이터를 기반으로 일/시/분/초 별 저가/고가/시가/종가 데이터 집계\n집계된 데이터를 PostgreSQL 에 저장\n이 당시 valuesight 서버는 매일 새벽마다 서버가 다운되어서 개발자가 수동으로 재기동시키고, 비어있는 데이터를 크롤링을 통해서 매우고 있었습니다. valuesight WAS 코드에는 동기화/락 코드가 있었기 때문에 레피니티브에서의 시세 데이터 전송 속도를 개별 I/O 처리 작업이 따라가지 못하면서 어느 순간에는 더 이상 처리할 수 없는 상태에 자주 빠지게 되었습니다.저의 경우에는 레거시 데이터 처리 서버인 valuesight 의 운영에 대한 어떤 이슈가 있는지에 대한 정보를 거의 전달받지 못하고 신규버전 출시 2주일 전에 코드만 전달 받은 채로 프로젝트를 넘겨받아서 시작했습니다.레거시 서버의 신규 서버 전환 작업을 맡으면서 미리 개발자에게 언급 없이 레피니티브의 데이터 구독제를 바꿔서 상용에서 장애가 난 경험도 있었습니다. 이때 1주일간 로그를 추출한 후 필드매핑을 처음부터 일일이 한땀 한땀 떼어서 확인해서 다음날 미국장에 잘 되는지 확인해보던 잠 못자고 살 떨리던 경험도 있었고, M증권사의 개발팀 부장님과 컨택이 되서 구글 밋으로 회의를 하면서 실시간 데이터를 어떻게 처리하는 지에 대한 팁도 얻었던 경험이 있습니다.","overview#Overview":"미국 주식 시세 데이터를 처리하기 위해서 단계적으로 스레드 프로그래밍이 개선을 거듭했고, 최종적으로는 아래와 같은 구조가 되었습니다. 랩장님과 인프라팀의 인원들의 반대로 인해 도입하지 못한 기술적인 면들이 많았지만, 시간내에 프로젝트를 완료시킬 수 있는 최적의 구조는 아래와 같았습니다.","서비스-역할-정의#서비스 역할 정의":"\"메시지 큐를 단순히 소비자와 생산자를 두는 것이 단순히 처리를 뒤로 미루는 용도로 사용하지 않아야 한다\"는 점에 집중해서 작업을 처리하는 구조와 메시지 소비 구조를 맞춰나가는 작업을 해왔습니다.","waiker-data-live#waiker-data-live":"레피니티브의 미국 주식 시세 데이터는 그 트래픽의 속도가 굉장히 빠릅니다. 이것을 온전히 데이터를 큐잉 하는 작업만으로도 부하가 상당했습니다. 이런 이유로 레피니티브의 raw 데이터를 우리팀의 구조에 맞춰서 변환하는 작업과 RabbitMQ에 지연 없이 전송이 가능하는 역할만을 전담하는 인스턴스인 waiker-data-live 를 별도로 두었습니다.\n레피니티브 raw 데이터 필드맵 변환\n데이터타입 정형화\n변환한 데이터를 RabbitMQ에 큐잉\nwaiker-data-live 는 서비스의 가장 앞단에서 원본 데이터를 서빙하는 역할을 합니다.레피니티브의 미국 주식 트래픽이 꽤 높기 때문에 앞단에서 원본 데이터만 서빙하는 역할의 서버를 따로 분리해야 했습니다.\n데이터 타입 정형화이 당시 레피니티브의 공식 문서를 찾거나 레피니티브 개발자 포럼에 질문을 남겨서 특정 필드가 나노세컨드 단위인지 밀리세컨드인지, 정수형 타입, 실수형 타입에는 어떤 것들이 있는지를 모두 확인해서 맞춰둔 후에 데이터타입을 포매팅하는 과정을 거쳤습니다.혹시라도 이 글을 읽는 분들 중 레피니티브 데이터 개발 업무를 수행하는 데에 도움이 될 수도 있는 분들이 있을것 같아서 그 코드의 일부를 남겨보면 아래와 같습니다.\n@Slf4j\r\n@Getter\r\npublic class FieldEntryUtils {\r\n    // ...\r\n    \r\n\tprivate final Function<FieldEntry, String> fieldEntryToStr_Function = (fieldEntry -> {\r\n        // 자체 정의한 switch ~ case 문에 존재하지 않는 타입도 String 으로 일단 받아서 확인하기 위해 추가\r\n\t\tString str = fieldEntry.load().toString(); \r\n\r\n\t\tswitch (fieldEntry.loadType()) {\r\n\t\t\tcase DataTypes.UINT, DataTypes.UINT_1, DataTypes.UINT_2, DataTypes.UINT_4, DataTypes.UINT_8 -> {\r\n\t\t\t\tlong l = fieldEntry.uintValue();\r\n\t\t\t\tstr = String.valueOf(l);\r\n\t\t\t}\r\n\t\t\tcase DataTypes.INT -> {\r\n\t\t\t\tlong l = fieldEntry.intValue();\r\n\t\t\t\tstr = String.valueOf(l);\r\n\t\t\t}\r\n\t\t\tcase DataTypes.REAL, DataTypes.REAL_4RB, DataTypes.REAL_8RB -> {\r\n\t\t\t\tdouble v = fieldEntry.real().asDouble();\r\n\t\t\t\tstr = String.valueOf(v);\r\n\t\t\t}\r\n\t\t\tcase DataTypes.DOUBLE, DataTypes.DOUBLE_8, DataTypes.FLOAT, DataTypes.FLOAT_4 -> {\r\n\t\t\t\tdouble v = fieldEntry.doubleValue();\r\n\t\t\t\tstr = String.valueOf(v);\r\n\t\t\t}\r\n\t\t\tcase DataTypes.DATE -> {\r\n\t\t\t\tif(fieldEntry.date().day() == 0) {\r\n\t\t\t\t\tstr = LocalDate.of(fieldEntry.date().year(), fieldEntry.date().month(), 1).format(ServerStockTime.formatter);\r\n\t\t\t\t}\r\n\t\t\t\telse{\r\n\t\t\t\t\tLocalDate date = LocalDate.of(fieldEntry.date().year(), fieldEntry.date().month(), fieldEntry.date().day());\r\n\t\t\t\t\tstr = String.valueOf(date.format(ServerStockTime.formatter));\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tcase DataTypes.TIME -> {\r\n\t\t\t\tLocalTime time = LocalTime.of(fieldEntry.time().hour(), fieldEntry.time().minute(), fieldEntry.time().second(), fieldEntry.time().nanosecond());\r\n\r\n\t\t\t\tlong l = time.toNanoOfDay();\r\n\t\t\t\tstr = String.valueOf(l);\r\n\t\t\t}\r\n\t\t\tcase DataTypes.DATETIME -> {\r\n\t\t\t\tstr = fieldEntry.dateTime().toString();\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\treturn str;\r\n\t});\r\n}\n데이터 구조일반적인 상용 서비스나 현실세계의 데이터가 아니어서 이해는 어렵겠지만, 당시에 사용했던 데이터의 구조는 아래와 같았습니다.\npublic class FieldEntryDto implements Serializable {\r\n    private String ric;\r\n\tprivate String nbRic;\r\n\tprivate Map<String, String> fieldMap;\r\n\tprivate SymbolDto symbolDto;\r\n    // ...\r\n}\n그 당시 사용하던 Java 16 에서는 record 키워드가 preview 였고, 이 기능에 record 를 기반으로 전환할 지 고민을 거듭하다가 일반 class 기반으로 두었는데, 아마도 프로젝트를 다시 한다면 record 기반으로 구현하지 않았을까 싶습니다.그 당시 메시지 큐를 카프카를 사용하는 것이 아니었기에 파티셔닝에 대해 고민하지 않았다는 점은 확실히 장점이었던 것 같습니다.\n아쉬웠던 점waiker-data-live 의 경우 상태가 있는 서버 애플리케이션이 아니기에 EKS로 전환이 쉽고, 저 역시도 EKS 전환이 시급하다고 느껴왔던 프로젝트였지만, 이 당시 개발 기한문제도 있었고 EKS를 운영할 데브옵스 팀의 인력문제로 인해 k8s 네이티브 앱으로 전환하지 못한점이 가장 큰 아쉬움으로 남는 서비스입니다.","waiker-data-collector#waiker-data-collector":"waiker-data-collector 는 data-live-queue 에서 데이터를 수신한 후 이 데이터를 일/시/분/초 단위로 시가/고가/종가/저가 를 집계하는 역할을 수행합니다. 그리고 집계된 데이터를 일정 주기별로 RabbitMQ 에 send 하거나 PostgreSQL 에 upsert 를 수행합니다.\nRabbitMQ data-live-queue 의 데이터 수신\n데이터 필드 매핑 및 변환\n데이터의 일/시/분/초 별 시가/고가/종가/저가 집계\n현재가 기반 데이터를 별도의 웹소켓 데이터로 변환 작업 및 캐싱\n집계된 데이터를 RabbitMQ websocket-queue Push\n집계된 데이터를 PostgreSQL upsert\nRabbitMQ data-live-queue 의 데이터를 수신하는 작업은 데이터의 트래픽이 높기에 별도의 스레딩 처리가 필요했습니다. 이것을 위해 리스너에서 데이터를 수신하는 역할을 하는 listenerExecutor 라는 별도의 ExecutorService 를 별도로 선언해두었고 RabbitMQ 로부터의 데이터의 수신작업은 모두 람다 바디 안에서 이뤄지게끔 했습니다.데이터의 필드 매핑 및 변환 작업은 기술적으로 난이도가 높지는 않았지만, 주식 데이터의 필드 매핑을 파악하는 작업이 쉽지 않았습니다. 실제 raw 데이터의 필드 중 어떤 것이 실제 거래 가격인지 파악하는 것 부터 어떤 필드를 시간필드로 사용해야 하는지 등을 일일이 로그 파일을 json 덤프파일을 통해 정형화 후 테스트 코드 위에서 개발하는 작업을 거쳤습니다.데이터의 일/시/분/초 별 시가/고가/종가/저가 집계 작업은 난이도가 쉬운편에 속했습니다. 특정 시간 영역 내에서 고가보다 높은 값이 들어오면 고가를 갱신하고 저가보다 낮은 가격이 들어오면 저가를 갱신하고, 특정 시간 영역 내의 종가를 계속 업데이트하는 등의 연산이었기에 구현의 난이도는 쉽지 않았습니다.집계된 데이터를 RabbitMQ websocket-queue Push 작업을 하는 것은 쉽지 않았습니다. RabbitMQ로 데이터를 전송하는 네트워크 로직 자체가 네트워크 통신 로직이기에 블로킹 로직이고 이 블로킹 로직들을 적절한 사이즈의 스레드 풀 내에서 비동기 처리되게끔 해야 했습니다. 웹소켓 데이터의 경우 사용자에게 보여지는 항목이고 DB에 데이터 저장하는 것 보다는 어느 정도는 데이터를 유실되고 최신 데이터를 보내는 것에 중점을 둬야 한다는 것에 초점을 두었고, 오래된 데이터는 ttl 정책에 의해 캐시에서 제거되도록 하고, 일정시간 (0.01ms) 마다 데이터가 있는지를 검사해서 데이터가 있을 경우에만 List 단위로 데이터를 전송하도록 했습니다.현재가 기반 데이터를 별도의 웹소켓 데이터로 변환 작업 및 캐싱을 하는 작업은 난이도가 높지 않았습니다. 다만, 웹/앱에서 원하는 데이터의 형식을 조율하거나, nbRic, oric 의 요구사항 변경에 따른 로직 변경, 해당 주식 거래소의 현지시각으로 데이터의 거래시각 변경 후 전송 등 기획상에서 자주 바뀌는 부분들에 대해 대응하는 부분들이 많았습니다.집계된 데이터를 PostgreSQL upsert 하는 작업은 비교적 비동기 작업의 처리가 수월했습니다. 현재가가 1분에도 수십번 이상 바뀌는 증권데이터 특성상 MVCC 작업으로 인해 PostgreSQL 내에서 Dead Tuple 이 기하급수적으로 늘어나는 현상이 있었습니다. 이 당시 MVCC 성능에 부하가 있더라도 장중 트래픽은 어느정도 느슨했기에 별 이슈는 없었지만, 장시작후 15분간, 장마감 전후 15분간은 MVCC 성능 저하로 인해 DB 데이터 저장에 레이턴시가 생겼고 처리 속도가 느려지는 이슈가 있었습니다. 애플리케이션의 스레드나 메모리를 Grafana 로 보더라도 웹 애플리케이션에는 이슈가 없었지만 DB의 MVCC 관련 이슈로 인해 PostgreSQL 에 Vaccum 작업을 필요한 시기에 제때 제때 해줬어야 했던 이슈가 있었습니다.","waiker-data-websocket#waiker-data-websocket":"waiker-data-websocket 은 websocket-queue 의 데이터를 받아서 온전히 데이터를 웹/앱 으로 전송하는 역할만을 담당하도록 했습니다.초기 개발 시에는 waiker-data-collector 와 waiker-data-websocket 사이에 기능의 경계선이 모호했지만, 개발을 진행하면서 점점 waiker-data-websocket 은 데이터의 물리적인 전송에만 초점을 맞추도록 서비스를 구성했습니다.이 당시 EKS 운영하고 관리할 데브옵스 인력이 없어서 EKS 기반의 k8s 앱으로 전환하지 못한 점은 아쉬웠다고 생각합니다.데이터를 전송하는 역할에만 초점이 맞춰져있지만, 레피니티브의 시세 데이터를 온전히 소모해야 하는 역할과 갈수록 많아지는 사용자들에게의 데이터 전송하는 역할로 인해 전체 서비스 중 waiker-data-websocket 서비스가 가장 부하가 심했다고 생각합니다.","아쉬웠던-점#아쉬웠던 점":"","생산자소비자-기반-동시성-처리-구조-커스터마이징#생산자/소비자 기반 동시성 처리 구조 커스터마이징":"그 당시 인스턴스 각각은 Producer, Consumer, Queue 로 구성했습니다. 여기서 Queue 는 Hazelcast 를 활용했고 spring-data-hazelcast 를 통해서 각 요청을 Set, Map<String, List<T>> 을 통해서 FIFO Queue 를 흉내낸 Write-Back 캐싱전략을 구현했습니다.프로젝트를 다시 한다면, 데이터 가 아닌 요청 을 기록하는 용도의 테이블을 따로 만들어서 기록을 하는 구조로 전환하게 될 것 같습니다. 동시성 처리가 되어 있는 생산자/소비자 기능은 개별 요청을 기록(Insert) 하고, 주기적으로 이 테이블에서 벌크단위로 기록된 요청 데이터를 읽어들여서 시세테이블에 데이터를 적재하도록 구조를 전환해서 동일행 수정으로 인한 DBMS에서의 잦은 MVCC 이슈를 피하고, 각각의 요청을 고유하게 기록할 수 있는 구조로 전환하게끔 해보고 싶습니다.","tsdb#TSDB":"1차적인 데이터 처리 용도의 데이터베이스는 관계형 데이터베이스 대신 시계열 데이터 기반의 데이터베이스를 적용했을 듯 합니다.관계형 데이터베이스를 억지로 시계열 데이터에 처음부터 맞추는 것은 억지스럽다는 생각은 했으나 당시 개발 여건 상 팀내의 입지나 여러모로 도입하기가 쉽지 않았습니다. 관계형 데이터베이스는 데이터를 중복을 없애고 일관성있는 논리적인 데이터의 구조를 통해 데이터를 효율적으로 저장하는 것이 목적이기에, 현재가/고가/저가/종가가 계속 변할 때마다 동시성 이슈가 크다고 생각했습니다.아마도 프로젝트를 다시 한다면 관계형 데이터베이스에는 최종적으로 데이터를 1분마다 집계한 최종 데이터를 저장하고, TSDB에 시세데이터를 증분기록하게끔 할 것 같습니다. 지금은 이렇게 적긴 했지만, 시세 데이터 자체가 관계형 데이테베이스에 저장한다는 것 자체가 저 자신은 가끔 이해가 안되긴 하지만, 팀내에서 이렇게 하기를 원한다면 관계형 DB에 저장을 하게 될 듯 합니다.TSDB(InfluxDB 등) 외에도 MongoDB, ElasticSearch 가 대안이 될수 있을 것 같고, 완전하게 독자적으로 책임질수 있는 여건이라면 관계형 Database 에 시세를 기록하지는 않을 듯 합니다. 대신 시세 조회시에는 API를 통해서 조회를 하게끔 하고, 시세 서비스 내에서는 NoSQL 데이터베이스를 사용하게끔 했을 듯 합니다."}},"/architecture-experience/test-case-experience":{"title":"Test Case Experience","data":{"테스트케이스-경험#테스트케이스 경험":"","필드-매핑-파악의-어려움--로그-추출--필드매핑-테스트-코드#필드 매핑 파악의 어려움 : 로그 추출 & 필드매핑 테스트 코드":"레피니티브로부터 전달되는 시세 데이터는 아래와 같은 형식으로 전달됩니다.각각의 필드가 어떻게 나타나는지는 우리나라의 개발자 플랫폼 처럼 친절하게 제공되는 가이드 문서는 없으며 개발자가 일일이 개발자 포럼을 일일이 찾아서 파악을 해야합니다. 당시 개발 기한도 빠듯했지만, 이 필드 매핑을 따로 파악해줄만한 인력이 없었습니다. 가끔씩 주식 운용을 담당하는 내부 직원 분이 초봉,분봉을 보고   이 필드가 현재가이고 이 필드를 사용하면 단된다거나 이런 내용들을 봐주시기도 했는데, 개발자가 파악했을 때보다 가끔 정확했을 때가 있어서 가끔 놀랐던 것 같습니다.그리고 위에서 나타나는 각각의 필드는 어떤 때는 나타나고 어떤 때는 나타나지 않습니다. 전달 받는 데이터가 어떤 거래 유형인지에 대해서도 제공되지 않기에 로직 내에서 거래 유형을 파악하는 로직들을 코드로 직접 작성해왔습니다.\n위와 같은 거래데이터들을 필드 매핑을 하기 위해서 아래와 같이 로그 파일로 추출했습니다.\n그리고 이 로그 내에서 실제 서비스에서 필요한 필드들만 추려내기 위해 객체를 정의하고 이 것을 json 으로 변환하는 작업을 거쳤습니다.\n그리고 이 필드 들 주에서 어떤 것이 거래 시각이 맞는지, 어떤 것이 거래가격인지를 보고 시간단위등도 함께 고려해서 레피니티브 개발자 포럼의 필드 설명에 맞춰서 어떤것이 맞는지 대조를 하는 과정을 직접 해왔습니다. 이렇게 해서 찾아낸 필드 매핑들은 아래와 같이 테스트 코드 작성을 통해 명세화 하고 멱등성을 갖도록 해줬습니다.","tick-집계-코드-테스트-코드-검증#Tick 집계 코드 테스트 코드 검증":"레피니티브로부터 전달되는 데이터는 시가/고가/저가/종가가 갖추어진채로 전달되는 경우도 있고, 단순히 현재가만 전달해주는 경우도 있습니다. 처음 개발을 시작할 때에는 이런 데이터 유형이 보통 어떤 시점에 오는지를 몰랐기에 두가지 테스트 케이스를 모두 작성했습니다. 시가/고가/저가/종가 테스트코드\n시가/고가/종가/저가 데이터 없이 현재가만 메시지로 오는 경우에 대한 데이터 매핑나중에 운영 경험이 3개월 이상 쌓이면서 부터 시가/고가/저가/종가를 같이 메시지로 보내주는 경우는 확률적으로 개장 전후 5분내외, 장 마감 전후 5분 내외에 발생한다는 사실을 알게되었습니다.이 외에도 각종 시/고/저/종 가격을 집계하는 코드들에 대해 모두 단위 테스트 코드를 모두 일일이 만들어두어서 테스트 케이스화 하고, 명세화해서 해당 기능의 확인과 검증을 쉽게했고, 배포 시에 자신감을 갖고 배포할 수 있게 되었습니다.","비동기-로직의-호출-여부-테스트-코드-검증#비동기 로직의 호출 여부 테스트 코드 검증":"listener 로직에서는 ExecutorService 기반으로 tick 집계 로직을 수행했습니다. tick 집계시에 IO 작업은 캐시에 쌓아두기만 하기에 응답의 지연은 없었고, ExecutorService 의 동작이 올바르게 동작하는지를 검증하기 위해 아래와 같은 코드를 작성했습니다.당시에는 builder 를 사용했었는데, 현시점에서 다시 돌아봤을 때, 테스트 코드 작성시 필요한 TestFixture 를 테스트 스코프에 별도로 테스트 케이스마다 구현을 해두어서 별도로 테스트 케이스를 정의했을 것 같습니다.","docker-compose-testcontainer-를-이용한-database-코드-검증#docker-compose, testcontainer 를 이용한 Database 코드 검증":"docker-compose 를 testcontainer 를 기반으로 구동해서 DAO 로직의 테스트를 검증했었는데 그 테스트 코드는 아래와 같습니다.\n이렇게 모든 실시간 데이터 처리 작업을 하는 데이터 모듈들은 아래와 같이 모두 테스트코드화 했습니다.\n그리고 위의 코드에서 사용하는 docker-compose.yml 파일은 아래와 같습니다.","전반적인-테스트-코드들#전반적인 테스트 코드들":"이 외에도 굉장히 많은 테스트 코드들을 작성해두었는데, 아래와 같이 굉장히 많은 테스트 코드를 남겨두었고 대부분 어떤 특정 상태에 의존되지 않도록 하면서 배포 전에 모든 테스트를 돌려봤을 때 실패하는지를 1차적으로 검증할 수 있는 간편한 수단이었습니다. 이렇게 해두면서 1인 개발 체제에서 힘에 부치던 상황에서 배포하려는 기능에 대해 자신감을 가지게 되었던 기억이 있습니다."}},"/change-history":{"title":"Change History","data":{"문서-변경-이력#문서 변경 이력":"2024.04.08\narchitecture-experience/if-project-again.mdx, message-queue-usage.mdx , fyi.mdx\n내용 간소화 및 잘못 요약했던 내용 수정\nExactly Once 내용 추가, TSDB 관련 내용추가, 동시성 처리 구조 내용 수정\nchange-history.mdx : 문서변경 이력 추가"}},"/fyi":{"title":"Fyi","data":{"fyi#FYI":"","공백기간-동안#공백기간 동안":"공백기간 동안에 주로 Spring Webflux, Kotlin Coroutine, Kafka, EKS 등에 대해 스터디하고 문서화해두었습니다. 직장에서 레거시 서비스를 운영하면서 시간을 내서 스터디 하지 못할 내용들을 스터디하는 시간을 갖는게 조금은 더 나을 수도 있겠다고 생각해서였습니다. Webflux, Kotlin Coroutine 에 대해 스터디한 내용들의 문서를 정리하면서 현재는 Servlet 기반의 Tomcat 이 서비스의 주류를 이루지만, 5년 안에는 Netty 기반의 서버 컨테이너에 관심이 전환될 것 같다는 생각이 들었습니다. 모바일 환경의 트래픽을 감당하기에는 Servlet 방식의 서버 컨테이너는 스레드 하나가 커넥션 하나를 담당하거나, IO 작업 하나를 담당하기에 리소스의 비용이 꽤 비싸며 운영 비용도 점점 비싸집니다. 직접 스터디하면서 알게된 Netty  컨테이너는 조금은 더 경량화된 프레임워크라는 것을 느꼈습니다. 스레드에 대해 여러작업을 이벤트 기반의 채널링을 통해 비동기 논 블로킹 기반의 통신을 하기 때문입니다.이 외에도 운영업무를 하면서 지나쳤던 데이터베이스에 대한 사소한 개념들까지도 문서화를 하게 되었습니다. 운영업무를 할 때는 바쁘다는 핑계로 지나쳐왔던 개념들을 새로 정리하게 되었던 것 같습니다.\n공백기간 동안에는 전 직장인 웨이커에서 힘든경험도 했고 가끔은 억울하다는 생각도 자주 했었지만, 지금 돌아보면, 조금 더 겸손해질수 있었던 것 같고, 과거를 돌아보면서 나 자신도 경솔한 적이 없었는지도 돌아보게 된 것 같아서 자기 치유의 시간 역시 가지게 되었다고 생각합니다.","업무-강점-해보고-싶은-일#업무 강점, 해보고 싶은 일":"스파게티 처럼 꼬인 레거시를 테스트 기반으로 명세화해서 새로운 서비스로 개편하는 업무에 자신이 있습니다. 만약 이런 일이 있다면 다시 한번 더 맡아보고 싶습니다. 실제로 전 직장에서 직접 책임지고 프로젝트 완수까지 했었기에 단순히 이론적인 접근이 아닌, 실무적으로 정해진 시간에 성과를 낼 수 있는 심리적인 면과, 일정조율, 커뮤니케이션 시에 장점이 될 것 같습니다. 항상 흐트러지지 않고 꾸준하게 노력하는 모습을 통해 다른 동료들에게 동기부여를 줄 수 있는 팀원이 되고 싶습니다. 기회가 된다면 한달에 한번 씩은 세미나는 아니더라도 조그마하게 회의실에서 그동안 자기 개발해왔던 내용들을 공유해서 팀의 집단지성이 발전할 수 있도록 기여해보고 싶습니다.최근 ELK, EFK 등에 관심이 많고, 업무를 추적하기 위한 ETL (Extract - Trasform - Load) 과 같은 데이터 처리에도 관심이 많아지고 있어서 기회가 된다면 비슷한 업무들에 도전해보고 싶습니다.저는 개발 일을 하지 않으면서 개발자인 것 처럼 경력을 쌓아서 포장하고 싶지 않습니다. 연봉 욕심도 없습니다. 개발업무에 대해 잘 이해하고 있는 실무진과 함께 합리적으로 일을 해보고 싶습니다. 이거 하나면 될 것 같습니다.","eof#EOF":"이 글을 읽어주시는 분들 중 저보다 실력과 경험이 뛰어나신 시니어 개발자 분들이 많으시겠지만, 혹시라도 제 글에 모자람이 있었다면 조금이나마 너그러이 양해부탁드립니다."}},"/":{"title":"Introduction","data":{"":"Ctrl + 마우스 휠 아래 스크롤 을 통해서 페이지 확대 비율을 90%로 축소해서 맞춰주시면, 훨씬 더 깔끔하게 보입니다."}},"/portfolio":{"title":"Portfolio","data":{"포트폴리오#포트폴리오":""}},"/portfolio/portfolio":{"title":"Portfolio","data":{"":"Spring Webflux, Spring Data R2dbc, Mongodb Reactive, Kotlin Coroutine\nhttps://chagchagchag.github.io/docs-spring-webflux/\nhttps://chagchagchag.github.io/docs-spring-data-reactive/\nhttps://chagchagchag.github.io/docs-kotlin-coroutine/\nhttps://chagchagchag.github.io/docs-webflux-spring-security/\nhttps://chagchagchag.github.io/docs-spring-reactive-test/ (진행 중)\nMySQL\nhttps://chagchagchag.github.io/docs-mysql-essential/\nk8s, EKS, ArgoCD\nhttps://chagchagchag.github.io/docs-fibonacci-backend/\nhttps://chagchagchag.github.io/docs-argocd-setup-at-eks/\nhttps://chagchagchag.github.io/argocd-rollout-deploy-docs/\nJava\nhttps://chagchagchag.github.io/docs-study-java/\netc\nRedis,MySQL 기반 쿠폰발급기 : https://chagchagchag.github.io/docs-coupon-service/\nEDA 기반 Spring Cloud : https://chagchagchag.github.io/eda-based-spring-cloud-doc/\nDDD, Clean Architecture : https://github.com/chagchagchag/fastcampuspay-v1 (진행 중)"}},"/project-history":{"title":"Project History","data":{"프로젝트-수행-기록-경력기술서#프로젝트 수행 기록, 경력기술서":""}},"/project-history/project-history":{"title":"Project History","data":{"프로젝트-수행-기록#프로젝트 수행 기록":"","waiker#waiker":"재직기간 : 2021.06 ~ 2022.06","waiker-증권-플랫폼-구축#waiker 증권 플랫폼 구축":"프로젝트 기간 : 2021.06 ~ 2021.11\nJava, SpringBoot, JPA, JdbcTemplate, Querydsl, Postgresql, Redis\nvaluesight 서비스 종료에 따른 신규 서비스 개발\n투자대가들의 점수, AI 스코어 등 주식 탭 API 및 데이터 개발\n테이블 설계, REST API 신규개발, QA, 내부 로직 고도화","레피니티브-증권-시세-데이터-처리-시스템-구축#레피니티브 증권 시세 데이터 처리 시스템 구축":"프로젝트 기간 : 2021.12 ~ 2022.06\nJava, SpringBoot, Spring AMQP, RabbitMQ, Postgresql, Java, JdbcTemplate, JPA,\r\nHazelcast\n레피니티브 시세 데이터 트래픽 처리 (미국, 중국)\n고빈도 트래픽(2.5k/s ~ 7.5k/s)의 I/O 처리 효율화 설계/개발/리팩토링 (웹소켓, 데이터저장)\nRAW 데이터 매핑, 추출, 테스트 케이스 작업\n프로젝트 전반적으로 테스트 커버리지 80% 가 되도록 테스트 커버리지 작업 수행 및 리팩토링","디케이테크인#디케이테크인":"재직기간 : 2019.11 ~ 2021.06","멜론-음원-시스템-서비스-운영#멜론 음원 시스템 서비스 운영":"운영 기간 : 2019.11 ~ 2021.06\nJava, Javascript, JQuery, Kotlin, Spring, SpringBoot, Mybatis, JPA, Memcached\n멜론 HiFi, 멜론 댓글, 댓글 어드민, 멜론 어드민, 카카오뮤직 등의 내부 업무들 티켓 기반 유지보수/운영\n레거시 개편 업무","누리플렉스#누리플렉스":"재직기간 : 2018.10 ~ 2019.11","sk-es-에너지-모니터링-시스템-구축#SK E&S 에너지 모니터링 시스템 구축":"프로젝트 기간 : 2019.04 ~ 2019.11\nJava, Javascript, JQuery, Spring, SpringBoot, Amchart, Oracle, MySQL, Mybatis\n에너지 모니터링 시스템 구축\n장비현황, 장비이력 기능 백엔드, 프론트엔드 개발 담당\nEMS/TOC 사업장 대시보드 개발","sustera-pv-ess-모니터링-시스템-구축#SUSTERA PV ESS 모니터링 시스템 구축":"프로젝트 기간 : 2018.12 ~ 2019.02\nJava, Javascript, JQuery, Spring, Amchart, Oracle, MySQL, Mybatis\n에너지 모니터링 시스템 구축\n장비현황, 장비이력 관련 백엔드, 프론트엔드 개발 담당","cj-pcc-부산-신재생에너지-혁신센터-에너지-모니터링-시스템-운영#CJ PCC 부산 신재생에너지 혁신센터 에너지 모니터링 시스템 운영":"운영 기간 : 2018.10 ~ 2019.11\nJava, Javascript, JQuery, Spring, SpringBoot, AngularJS, Amchart, Morris Chart, Oracle, MySQL, Mybatis\n웹소켓 장애 대응, Frontend 유지보수\nESS 충방전 현황 대시보드 개발","주니코리아#주니코리아":"재직기간 : 2015.05 ~ 2017.04","hardware-replacement-telsta#Hardware Replacement (TELSTA)":"프로젝트 기간 : 2015.09 ~ 2016.01\nJava, TCP/IP, SWT/JFace\nLTE 중계기 관제솔루션\nTCP/IP 통신 로직 & 화면 렌더링 로직 개발","프리랜서#프리랜서":"","블록체인-서비스-운영-프리랜서#블록체인 서비스 운영 (프리랜서)":"2023.06 ~ 2023.08\nKotlin, Springboot, MySQL, RestDocs, Ganache, Web3j, ethereum\n금융권 블록체인 서비스 운영업무 (프리랜서)"}},"/work-history":{"title":"재직 기록","data":{}},"/project-history/problem-solving-histories":{"title":"Problem Solving Histories","data":{"경력기술서#경력기술서":"","웨이커-202106--202206#웨이커 (2021.06 ~ 2022.06)":"","신규-서비스-api-개발#신규 서비스 API 개발":"레거시 서비스(valuesight)의 종료로 인한 신규서비스(waiker)의 API 개발업무를 진행했습니다.대가들의 분석, 뉴스, 주요재무, AI종목분석, 종목정보 등 주식 탭에 존재하는 API 개발을 담당했고 주로 AI 개발자 분들의 분석 데이터, 크롤링 데이터를 쌓아두기 위한 테이블 설계, 웹/앱에서 데이터 조회를 위한 API 구현 등이 주된 업무였습니다.","레피니티브-증권-시세-데이터-처리-시스템-구축#레피니티브 증권 시세 데이터 처리 시스템 구축":"","프로젝트-설명#프로젝트 설명":"한국증권거래소 처럼 증권 시세를 Serving 하는 레피니티브라고 하는 회사의 시세 데이터를 1차 가공한 후 웹소켓 Push, Database 저장하는 것을 담당하는 시스템입니다.","트래픽-규모#트래픽 규모":"트래픽은 아래와 같이 2.5k/s ~ 7.5k/s 였습니다.\n개장(Market Open) 트래픽 추이\n폐장(Markt Close) 트래픽 추이","주요-문제#주요 문제":"2.5k/s ~ 7.5k/s 의 트래픽을 밀리지 않고 데이터를 저장하고, 웹소켓을 통해 웹/앱 으로 Push 를 해야 했습니다.웹소켓 푸시, 데이터 저장에 드는 I/O 작업의 시간 비용은 고정적으로 소모되므로, 일반적인 서버 애플리케이션의 처리방식과는 다른 접근방식이 필요했습니다. 또한, 요청이 유실 되었을 경우 이것을 복구해낼 방법이 필요했는데 이 부분에 대한 접근방식도 필요했습니다.","메시지-큐-기반-작업-분배-방식-도입#메시지 큐 기반 작업 분배 방식 도입":"작업의 성격별로 서버 애플리케이션을 3가지로 분류해서 아래와 같이 나눴고, 각각이 별도의 서버 애플리케이션으로 동작하도록 구성했습니다.\nwaiker-data-live\n레피니티브 raw 데이터 수신\n거래시각데이터를 LocalTime, LocalDate 기반 의 데이터로 변환\n변환된 RabbitMQ 메시지 큐에 데이터 적재\nwaiker-data-collector\n필드 매핑 작업 (가장 어려운 작업입니다.)\n일/시/분/초 단위 별 시가/고가/종가/저가 계산\noffheap 캐시 적재\nPostgreSQL 에 집계 데이터 저장\nwaiker-data-websocket\n웹/앱 단말로 웹소켓 푸시 작업 처리\n웹/앱 단말이 많아질 수록 부하가 높아지는 인스턴스","스케쥴링-기반의-소규모-배치작업-구조로-전환#스케쥴링 기반의 소규모 배치작업 구조로 전환":"코틀린의 코루틴이 스레드 하나를 여러개의 코루틴으로 나누어서 CoroutineDispatcher 가 코루틴의 실행/중지/재개/종료를 담당하는 것과 비슷한 원리를 스레드 풀 관리로직에 적용했습니다. ExecutorService 스레드 풀을 작게 잡고 이것을 스케쥴러 스레드를 이용해서 작업을 스케쥴링하고 각각의 작업을 CompletableFuture 를 이용해서 비동기 처리하는 방식으로 문제를 해결했습니다.","더-자세한-내용#더 자세한 내용":"웨이커에서 마지막으로 수행했던 증권 시세 데이터 처리 프로젝트는 별도의 설명이 필요해서 아래 링크에 더 자세한 내용을 확인하실 수 있습니다.\n시스템 설계 경험","디케이테크인-201911--202106#디케이테크인 (2019.11 ~ 2021.06)":"디케이테크인에서는 아래와 같은 업무들을 수행했습니다.\n뮤직 DNA 6.0 개편 : 월간/주간 개인 선호음악/추천음악 조회 백엔드 API 개발\n피드, 마이로그 6.0 개편 : 개인화 영역 소식 내역 메시지 생성 기능 개발\n멜론 댓글 서비스 운영/유지보수\n슬로우 쿼리 유지보수/대응\n카카오 클린플랫폼 연동 블랙리스트 기능 개발\n멜론 HiFi/음원/스테이션/카카오뮤직/계정 유지보수\n상용서비스를 개발하는 개발 유닛에서 함께 개발에 참여했고, 그 당시 생소한 개념이었던 Resilience4J 의 서킷브레이커 등의 Spring Cloud 개념들을 내부 개발 유닛의 개발 문서를 통해 접해보기도 했었고 카카오 클린 플랫폼의 블랙리스트 기능 연동시 이 기능을 적용했던 경험이 있습니다.디케이테크인에서 멜론 운영업무를 하면서 카카오 개발자 공채 출신 직원 분도 뵜었고, 멜론 출신 개발자 분들도 뵜었고, 기획자 분들도 뵜었고 이 외에도 굉장히 다양한 분들을 접했습니다. 멜론 개발 셀은 항상 스터디하는 습관이 베어있던 개발 조직이었기에 여기서 보고 배운 습관이 평소에도 기술 스터디를 꾸준히 하는 습관으로 이어졌습니다.","멜론-키즈-프로모션이-스테이션-배너에-랜딩되지-않던-이슈#멜론 키즈 프로모션이 스테이션 배너에 랜딩되지 않던 이슈":"스테이션 배너 앱 랜딩에 멜론 키즈의 프로모션이 랜딩되지 않는 이슈가 있었습니다. 멜론모바일, 멜론웹, 멜론 공통 모듈까지 모두 검사해서 어떤 부분이 잘못되었는지 찾아가는 과정을 겪었고, 결론은 멜론 어드민 내의 상수 코드 값이 빠져있어서 생기는 이슈였다는 것을 파악했습니다.결론은 굉장히 쉬워보이지만 워낙 많은 인원들의 입사와 퇴사를 거친 레거시 코드이기에 버그의 원인을 찾는데에 2주 반 정도 소요됐었고, 이 버그에 대한 히스토리조차도 없이 맨땅에서부터 시작해서 문제를 해결해나갔었기에 가장 어려웠던 경험이었습니다.","누리플렉스-201801--201911#누리플렉스 (2018.01 ~ 2019.11)":"누리플렉스에서는 아래와 같은 업무들을 수행했습니다.SK E&S STEP 에너지 모니터링 솔루션 개발/운영업무\n장비 개별정보 현황/이력(통계) 데이터 조회 API 설계/구현\n데이터 시각화 (차트라이브러리, 그리드 라이브러리 연동 등)\nSUSTERA PMS 에너지 모니터링 솔루션 개발/운영\n장비 개별정보 현황/이력(통계) 데이터 조회 API 설계/구현\n데이터 시각화 (차트라이브러리, 그리드 라이브러리 연동 등)\nCJ/부산 신재생 에너지 혁신센터 EMS 운영/유지보수\nMQ 데이터 트래픽 서버의 잦은 장애로 인한 SockJS 측에서의 예외처리\n이슈 및 개별 장애 대응\n위 업무 들 중 가장 기억에 남았던 문제해결 경험은 CJ/부산 신재생 에너지 혁신센터 EMS 운영/유지보수 시에 겪었던 MQ 데이터 트래픽 서버 장애 처리 경험입니다.","mq-데이터-트래픽-서버-장애에-대한-sockjs-타임아웃-처리#MQ 데이터 트래픽 서버 장애에 대한 SockJS 타임아웃 처리":"CJ/부산 신재생 에너지 혁신센터 시스템 운영 당시 개발팀에는 한전이나 LS로부터 전달받는 소켓 데이터를 Active MQ를 통해 웹소켓 데이터로 발송하거나 DB에 데이터를 INSERT하는 IO 작업을 담당하는 개발자 분이 계셨습니다. 한전/LS 로부터 전달받는 단건 데이터의 빈도에 비해 웹소켓/데이터저장 처리 속도가 물리적으로 느리기에 전기요금이 경부하 기간인 새벽시간 대에는 전기 충전 트래픽이 몰려 서버가 다운되는 현상이 있었습니다.이런 이유로 MQ서버 개발자 분께서는 야간에 자주 서버를 재기동하셨습니다.당시 저는 WAS 측의 운영을 담당하고 있었습니다. 서버 재기동이 영향을 주던 기능은 ESS 충방전현황 대시보드 내의 실시간 충전현황과 외기 온도를 보여주는 기능이었습니다. 서버를 재기동하기에 웹소켓 커넥션이 유실되는 것으로 인해 UI상으로는 관련된 기능이 다운된 것처럼 보이는 현상이 있었습니다.이 문제에 대해 ‘소켓 접속이 끊어지더라도 주기적으로 서버에 재접속 요청을 하도록 구성하는 것’ 이라는 점에 포인트를 주어 해결해야겠다는 결론을 내렸습니다. 그리고 SockJS 공식 문서를 참고해 SockJS의 내부 코드 중 디폴트 설정을 파악해서 디폴트 설정을 수정하고 기본으로 제공되는 SockJS 내부 코드의 일부분을 커스터마이징 해 문제를 해결했습니다. 수정했던 디폴트 설정은 ‘네트워크 커넥션 타임아웃 기간’, ‘재접속 Retry 주기’ 등 이었고, 무한대로 접속 요청을 하는 것으로 인한 부하 역시 줄여야 하기에 ‘재접속 횟수’에 제한을 걸어서 재접속 요청을 하도록 SockJS 소스코드를 수정해 배포했습니다.MQ 서버 처리로직에 안정성에 문제가 많았고 결함이 많았지만, 클라이언트 측(SockJS)의 코드를 유연하게 구성해서 제품의 문제가 발생하더라도 유연하게 장애에 대응했던 경험이라고 생각합니다.","주니코리아-201505--201704#주니코리아 (2015.05 ~ 2017.04)":"3G/4G 무선신호를 WIFI 로 변환해주는 기업용 데이터 네트워크 라우터들의 모니터링, 관리/제어를 위한 솔루션 유지보수/개발 업무를 해왔습니다.Hardware Replacement (TELSTRA)\nJAVA GUI, TCP/IP 통신 로직 개발\nHEMS 유지보수/개발/운영\nQA 대응, 유지보수/개발/운영, 트러블 슈팅","논블로킹-처리-경험#논블로킹 처리 경험":"Java GUI 애플리케이션에서 TCP/IP 통신 로직을 작성할 때 주로 외부 API를 호출할 때 GUI에 관련된 메인 스레드를 IO작업이 블로킹하지 않도록 로직을 구현하는 작업들이 많았습니다. 예를 들면 네트워크 설정파일(json, xml)의 SFTP Upload/Download 를 진행과 동시에 프로그래스바 UI에 진행률을 표시하는 기능을 구현하는 등의 작업 등을 해왔습니다."}},"/work-history/retire-reason":{"title":"Retire Reason","data":{"퇴직-사유#퇴직 사유":"","웨이커#웨이커":"업무의 과중함이 컸고, 신체적으로도 이상신호가 와서 별도의 이직 준비를 할수 있는 여건이 안되었기에, 이직 준비기간 없이 퇴직신청서를 내고 퇴사를 했습니다.가끔가다 정말 드물게 면접때 웨이커에서 회사와 싸우고 나왔는지 물어보는 분들이 가끔 계십니다. 퇴사 당시에 정말 너무 힘들어서 더 이상은 못하겠다고 대표님, 랩장님께 구구 절절하게 퇴직 사유란에 말씀드렸고, 대표님,랩장님 모두 미안해서인지 바로 OK 해주셨습니다. 이 외에도 회사 내부에서 거의 모든 직원분들이 모두 제가 힘들게 일하는 부분들을 알고 계셨습니다. 개인적으로 다시 떠올릴때마다 스트레스 받는 기억들이지만, 대부분의 면접관 분들이 지나치게 세세한 부분들까지 귀찮을 정도로 궁금해하시는 경우가 많았고 재밌어하시는 경우도 꽤 있어서... 적어둡니다.","디케이테크인#디케이테크인":"멜론 서비스를 운영하고 전방위적인 이슈들을 처리하고 해결해나가는 데에 소소한 기쁨을 느껴왔지만, 멜론 운영/개발 업무가 아닌 테스트 프레임워크 구축 등의 업무가 점점 늘어왔고 레거시 개편/운영보다는 새로운 프로젝트를 해보고 싶다는 생각이 들어서 웨이커에서의 이직제안을 수락하고 디케이테크인에서 퇴사를 하게 되었습니다.","누리플렉스#누리플렉스":"그 당시 2017 ~ 2019 년도 당시 ESS 신규사업들을 하던 사업부에서 신규 프로젝트를 2건을 해왔습니다. 이 당시 영업부서와 함께 일했기에 개발팀의 업무가 개별 영업 건에 자주  휘둘리는 점까지는 이해가 갔지만, 지나치게 잦은 회식으로 인해 업무에 큰 영향을 자주 받아왔습니다.적은 인력으로 프로젝트를 완수해가던 그 당시 환경으로서는 일정을 지키기 위해 수면도 꽤 부족했고, 체력적으로 부담이 커서 퇴사를 결정하게 되었습니다.","주니코리아#주니코리아":"JAVA 기반의 TCP/IP 통신기반의 프로젝트를 운영해왔는데, 진로를 백엔드 개발 분야로 결정하면서 퇴사를 결정했습니다."}},"/work-history/work-history":{"title":"Work History","data":{"재직기록#재직기록":"","웨이커#웨이커":"2021.06 ~ 2022.06\nBackend 개발\nJava, Spring Boot, JPA, Querydsl, JdbcTemplate, Redis, Hazelcast, RabbitMQ, Spring AMQP\n증권 분석 플랫폼 신규 개발\n레피니티브 증권 거래 데이터 처리 서버 개발 & 트래픽 IO 최적화","디케이테크인#디케이테크인":"2019.11 ~ 2021.06\nBackend 개발\nJava, Javascript, JQuery, Kotlin, Spring, SpringBoot, Mybatis, JPA, Memcached\n상용 시스템 운영 (멜론)\n지금은 서비스가 종료되었지만 멜론의 HiFi 담당자 였습니다.\n멜론 댓글, 댓글 어드민, 멜론 어드민, 카카오뮤직 등의 내부 업무들을 티켓 기반 유지보수/운영\n서비스 운영시 발생하는 수없이 많은 각종 버그들 처리, 다크모드 지원 작업 등등","누리플렉스#누리플렉스":"2018.10 ~ 2019.11\nBackend 개발\nJava, Javascript, JQuery, Spring, SpringBoot, AngularJS, Amchart, Morris Chart, Oracle, MySQL, Mybatis\n구축\nSUSTERA PV ESS 모니터링 시스템 구축\nSK E&S 에너지 모니터링 시스템 구축\n유지보수/운영\nCJ, 부산 신재생 에너지 혁신센터","주니코리아#주니코리아":"2015.05 ~ 2017.04\nJava 개발\nJava, TCP/IP, SWT/JFace\n개발\nHardware Replacement (TELSTA)\n네트워크 노드 모니터링/제어 시스템 운영/개발","프리랜서#프리랜서":"","블록체인-서비스-운영#블록체인 서비스 운영":"2023.06 ~ 2023.08 (계약종료)\n금융권 블록체인 서비스 운영업무(프리랜서)\nBackend 개발\nKotlin, Springboot, MySQL, RestDocs, Ganache, Web3j, ethereum","현재#현재":"결론적으로는 직전 회사(웨이커)를 잘못 입사해서 일을 힘들게 하고 나왔고, 정신적으로도 스트레스가 상당했고 그 이후로 몸이 안좋아져서 쉬었고 이래 저래 일이 안풀리다보니 더 꼬여서 잠깐 쉬었다가 재취업을 준비 중 입니다.재취업을 준비하는 동안 진심으로 노력하기 위해 노력했고, 취업을 한다면 지금 하고 있는 노력보다 10 배 더 노력해서 좋은 제품이 되도록 노력해보겠습니다.좋은 인연으로 만나뵐 수 있었으면 좋겠습니다."}}}